{
 "metadata": {
  "name": "",
  "signature": "sha256:629c2ae3a701643996a006b68e7d584b273b9876fc30946ac7e2e05ac1fec976"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Dataset: https://archive.ics.uci.edu/ml/datasets/Wine\n",
      "# 13 chemical measurements on 178 wines from 3 regions of Italy\n",
      "# Features are named (x1 x2 x3 x4 x5 x6 x7 x8 x9 x10 x11 x12 x13); labels are the regions (categorical)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import numpy as np\n",
      "from sklearn import cross_validation\n",
      "from sklearn.cross_validation import train_test_split\n",
      "from sklearn.cross_validation import KFold\n",
      "from sklearn import preprocessing\n",
      "from sklearn.pipeline import Pipeline\n",
      "from sklearn.svm import SVC\n",
      "from sklearn.grid_search import GridSearchCV\n",
      "\n",
      "labels=['class target', 'alcohol', 'malic acid', 'ash', 'alcalinity of ash', 'magnesium', 'total phenols', \n",
      "        'flavanoids', 'nonflavanoid phenols', 'proanthocyanins', 'color intensity', 'hue', \n",
      "        'OD280/0D315 of diluted wines', 'proline']\n",
      "target_={'class 1': 59, 'class 2': 71, 'class 3': 48}\n",
      "\n",
      "df = pd.read_csv('wine.data', names=labels, header=None)\n",
      "df_features = df.drop('class target', axis=1, inplace=False)\n",
      "df_target = df['class target']\n",
      "df.head()\n",
      "#df.info()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>class target</th>\n",
        "      <th>alcohol</th>\n",
        "      <th>malic acid</th>\n",
        "      <th>ash</th>\n",
        "      <th>alcalinity of ash</th>\n",
        "      <th>magnesium</th>\n",
        "      <th>total phenols</th>\n",
        "      <th>flavanoids</th>\n",
        "      <th>nonflavanoid phenols</th>\n",
        "      <th>proanthocyanins</th>\n",
        "      <th>color intensity</th>\n",
        "      <th>hue</th>\n",
        "      <th>OD280/0D315 of diluted wines</th>\n",
        "      <th>proline</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td> 1</td>\n",
        "      <td> 14.23</td>\n",
        "      <td> 1.71</td>\n",
        "      <td> 2.43</td>\n",
        "      <td> 15.6</td>\n",
        "      <td> 127</td>\n",
        "      <td> 2.80</td>\n",
        "      <td> 3.06</td>\n",
        "      <td> 0.28</td>\n",
        "      <td> 2.29</td>\n",
        "      <td> 5.64</td>\n",
        "      <td> 1.04</td>\n",
        "      <td> 3.92</td>\n",
        "      <td> 1065</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td> 1</td>\n",
        "      <td> 13.20</td>\n",
        "      <td> 1.78</td>\n",
        "      <td> 2.14</td>\n",
        "      <td> 11.2</td>\n",
        "      <td> 100</td>\n",
        "      <td> 2.65</td>\n",
        "      <td> 2.76</td>\n",
        "      <td> 0.26</td>\n",
        "      <td> 1.28</td>\n",
        "      <td> 4.38</td>\n",
        "      <td> 1.05</td>\n",
        "      <td> 3.40</td>\n",
        "      <td> 1050</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td> 1</td>\n",
        "      <td> 13.16</td>\n",
        "      <td> 2.36</td>\n",
        "      <td> 2.67</td>\n",
        "      <td> 18.6</td>\n",
        "      <td> 101</td>\n",
        "      <td> 2.80</td>\n",
        "      <td> 3.24</td>\n",
        "      <td> 0.30</td>\n",
        "      <td> 2.81</td>\n",
        "      <td> 5.68</td>\n",
        "      <td> 1.03</td>\n",
        "      <td> 3.17</td>\n",
        "      <td> 1185</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td> 1</td>\n",
        "      <td> 14.37</td>\n",
        "      <td> 1.95</td>\n",
        "      <td> 2.50</td>\n",
        "      <td> 16.8</td>\n",
        "      <td> 113</td>\n",
        "      <td> 3.85</td>\n",
        "      <td> 3.49</td>\n",
        "      <td> 0.24</td>\n",
        "      <td> 2.18</td>\n",
        "      <td> 7.80</td>\n",
        "      <td> 0.86</td>\n",
        "      <td> 3.45</td>\n",
        "      <td> 1480</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4</th>\n",
        "      <td> 1</td>\n",
        "      <td> 13.24</td>\n",
        "      <td> 2.59</td>\n",
        "      <td> 2.87</td>\n",
        "      <td> 21.0</td>\n",
        "      <td> 118</td>\n",
        "      <td> 2.80</td>\n",
        "      <td> 2.69</td>\n",
        "      <td> 0.39</td>\n",
        "      <td> 1.82</td>\n",
        "      <td> 4.32</td>\n",
        "      <td> 1.04</td>\n",
        "      <td> 2.93</td>\n",
        "      <td>  735</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 3,
       "text": [
        "   class target  alcohol  malic acid   ash  alcalinity of ash  magnesium  \\\n",
        "0             1    14.23        1.71  2.43               15.6        127   \n",
        "1             1    13.20        1.78  2.14               11.2        100   \n",
        "2             1    13.16        2.36  2.67               18.6        101   \n",
        "3             1    14.37        1.95  2.50               16.8        113   \n",
        "4             1    13.24        2.59  2.87               21.0        118   \n",
        "\n",
        "   total phenols  flavanoids  nonflavanoid phenols  proanthocyanins  \\\n",
        "0           2.80        3.06                  0.28             2.29   \n",
        "1           2.65        2.76                  0.26             1.28   \n",
        "2           2.80        3.24                  0.30             2.81   \n",
        "3           3.85        3.49                  0.24             2.18   \n",
        "4           2.80        2.69                  0.39             1.82   \n",
        "\n",
        "   color intensity   hue  OD280/0D315 of diluted wines  proline  \n",
        "0             5.64  1.04                          3.92     1065  \n",
        "1             4.38  1.05                          3.40     1050  \n",
        "2             5.68  1.03                          3.17     1185  \n",
        "3             7.80  0.86                          3.45     1480  \n",
        "4             4.32  1.04                          2.93      735  "
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Cross Validation of Non-Normalized features Linear SVM "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X = df_features.values\n",
      "y = df_target.values\n",
      "\n",
      "def cross_validate(X, y, classifier, k_fold=10):\n",
      "    \"Scores classifier using kfold cross_validation\"\n",
      "    # derive a set of (random) training and testing indices\n",
      "    k_fold_indices = KFold(len(X), n_folds=k_fold,\n",
      "                           shuffle=True, random_state=0)\n",
      "\n",
      "    k_score_total = 0\n",
      "    # train and score classifier for each slice\n",
      "    for train_slice, test_slice in k_fold_indices :\n",
      "        model = classifier(X[train_slice],y[train_slice])\n",
      "        k_score = model.score(X[test_slice], y[test_slice])\n",
      "        k_score_total += k_score\n",
      "\n",
      "    # return the average accuracy\n",
      "    return k_score_total/k_fold\n",
      "\n",
      "def describe_model(X, y, classifier, pipeline=0):\n",
      "    model = classifier(X, y)\n",
      "    print \"Class Values: {0}\".format(np.unique(y))\n",
      "    if not pipeline:\n",
      "        # shape = [n_class-1, n_SV]; For multiclass, coefficient for all 1-vs-1 classifiers\n",
      "        print model.dual_coef_.shape\n",
      "        # number of support vector for each class; shape = [n_class]\n",
      "        print model.n_support_\n",
      "        # for linear kernel; shape = [n_class-1, n_features]\n",
      "        print model.coef_.shape\n",
      "\n",
      "model = SVC(C=1.0, kernel='linear', probability=False).fit\n",
      "print \"CV Accuracy: {0}\".format(cross_validate(X, y, model))\n",
      "describe_model(X, y, model)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "CV Accuracy: 0.954901960784\n",
        "Class Values: [1 2 3]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "(2, 22)\n",
        "[5 9 8]\n",
        "(3, 13)\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "All classifiers in scikit-learn support multiclass classification out-of-the-box, this includes SVM with a linear kernel, as a one-vs-one approach.  By looking at the shape of the attributes, e.g. n_support, dual_coef_, the number of classes returned is 3, the same amount of unique values in the target."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Cross Validation of Normalized features Linear SVM"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Normalize the data\n",
      "# Scaled data has zero mean and unit variance\n",
      "# http://scikit-learn.org/stable/modules/preprocessing.html#standardization-or-mean-removal-and-variance-scaling\n",
      "\"\"\"\n",
      "normalizer = preprocessing.Normalizer().fit(df.values)\n",
      "data_normalized = normalizer.transform(df.values)\n",
      "X = data_normalized[:, 1:14]\n",
      "y = data_normalized[:, 0]\n",
      "cross_validate(X, y, SVC(C=1.0, kernel='linear', probability=False) )\n",
      "\"\"\"\n",
      "#normalizer = preprocessing.Normalizer().fit(X)\n",
      "#features_normalized = normalizer.transform(X)\n",
      "#features_normalized = preprocessing.scale(X)\n",
      "features_normalized = preprocessing.StandardScaler().fit(X).transform(X)\n",
      "model = SVC(C=1.0, kernel='linear', probability=False).fit\n",
      "print \"CV Accuracy: {0}\".format(cross_validate(features_normalized, y, model))\n",
      "describe_model(features_normalized, y, model)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "CV Accuracy: 0.955555555556\n",
        "Class Values: [1 2 3]\n",
        "(2, 22)\n",
        "[ 5 11  6]\n",
        "(3, 13)\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The Cross Validation accuracy is marginally better when normalized with zero mean and unit variance."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Pipeline (Normalization + Linear SVC)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Pipelines\n",
      "# http://scikit-learn.org/stable/modules/pipeline.html\n",
      "estimators = [('norm', preprocessing.StandardScaler()), ('svm', SVC(C=1.0, kernel='linear', probability=False))]\n",
      "model = Pipeline(estimators)\n",
      "# applying normalization via pipeline\n",
      "print \"CV Accuracy: {0}\".format(cross_validate(X, y, model.fit))\n",
      "describe_model(X, y, model.fit, 1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "CV Accuracy: 0.955555555556\n",
        "Class Values: [1 2 3]\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Vary parameters of the model for the pipeline"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Vary the 'C' parameter or the type of kernel\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0) \n",
      "\n",
      "# linear kernel, vary penalty parameter of the error\n",
      "penalties = [0.01, 0.1, 1, 100, 1000, 10000, 100000]\n",
      "for c in penalties:\n",
      "    estimators = [('norm', preprocessing.StandardScaler()), ('svm', SVC(C=c, kernel='linear', probability=False))]\n",
      "    model = Pipeline(estimators)\n",
      "    print \"C={0}\".format(c) \n",
      "    print \"CV: {0}\".format(cross_validate(X, y, model.fit))\n",
      "    # on test/train split, see what is misclassified\n",
      "    model.fit(X_train, y_train)\n",
      "    yhat = model.predict(X_test)\n",
      "    misclassified = ((yhat - y_test) != 0)\n",
      "    print \"Number of Misclassified:{0}\".format(np.count_nonzero(misclassified))\n",
      "print \n",
      "kernels = ['linear', 'poly', 'rbf', 'sigmoid']\n",
      "for kernel_idx in kernels: \n",
      "    estimators = [('norm', preprocessing.StandardScaler()), ('svm', SVC(C=1.0, kernel=kernel_idx, probability=False))]\n",
      "    model = Pipeline(estimators)\n",
      "    print \"Kernel Type={0}\".format(kernel_idx) \n",
      "    print \"CV: {0}\".format(cross_validate(X, y, model.fit))   \n",
      "    # on test/train split, see what is misclassified\n",
      "    model.fit(X_train, y_train)\n",
      "    yhat = model.predict(X_test)\n",
      "    misclassified = ((yhat - y_test) != 0)\n",
      "    print \"Number of Misclassified:{0}\".format(np.count_nonzero(misclassified))  "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "C=0.01\n",
        "CV: 0.983333333333\n",
        "Number of Misclassified:0"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "C=0.1\n",
        "CV: 0.988888888889\n",
        "Number of Misclassified:0\n",
        "C=1\n",
        "CV: 0.955555555556"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Number of Misclassified:0\n",
        "C=100\n",
        "CV: 0.961111111111\n",
        "Number of Misclassified:0\n",
        "C=1000\n",
        "CV: 0.961111111111"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Number of Misclassified:0\n",
        "C=10000\n",
        "CV: 0.961111111111\n",
        "Number of Misclassified:0\n",
        "C=100000\n",
        "CV: 0.961111111111"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Number of Misclassified:0\n",
        "\n",
        "Kernel Type=linear\n",
        "CV: 0.955555555556\n",
        "Number of Misclassified:0"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Kernel Type=poly\n",
        "CV: 0.961111111111\n",
        "Number of Misclassified:2"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Kernel Type=rbf\n",
        "CV: 0.983006535948"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Number of Misclassified:0\n",
        "Kernel Type=sigmoid\n",
        "CV: 0.399019607843"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Number of Misclassified:24\n"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The sigmoid kernel with no error penalty does not perform well at all (~0.40 CV), while the rbf (Gaussian radial basis function kernel) does, with a CV of 0.983.  The sigmoid hence all has a number of misclassified results between the prediction and test vector (24).  The poly kernel had a few misclassifications (2).  It would be good to try try the prediction over a different slice.  \n",
      "\n",
      "When looking at the penalty error over a linear kernel, it is clear that there does not appear to be an optimal C.  The CV is consistent from large to small values of C other than for C < 1.  So choosing small or large values of C did not improve the classification, and the comparision between prediction and test sets did not show any misclassified.  We disregard a tiny valy of C (e.g. 0.1), as this will probably lead to misclassification.   \n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Gridsearch"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Gridsearch\n",
      "# http://scikit-learn.org/stable/modules/grid_search.html\n",
      "# feed pipeline classifier to the grid search. \n",
      "# explore a range of values for C, gamma and the type of kernel. optimum value?\n",
      "estimators = [('norm', preprocessing.StandardScaler()), ('svm', SVC(C=1.0, kernel='linear', probability=False))]\n",
      "model = Pipeline(estimators)\n",
      "model.get_params()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 17,
       "text": [
        "{'norm': StandardScaler(copy=True, with_mean=True, with_std=True),\n",
        " 'norm__copy': True,\n",
        " 'norm__with_mean': True,\n",
        " 'norm__with_std': True,\n",
        " 'svm': SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, degree=3, gamma=0.0,\n",
        "   kernel='linear', max_iter=-1, probability=False, random_state=None,\n",
        "   shrinking=True, tol=0.001, verbose=False),\n",
        " 'svm__C': 1.0,\n",
        " 'svm__cache_size': 200,\n",
        " 'svm__class_weight': None,\n",
        " 'svm__coef0': 0.0,\n",
        " 'svm__degree': 3,\n",
        " 'svm__gamma': 0.0,\n",
        " 'svm__kernel': 'linear',\n",
        " 'svm__max_iter': -1,\n",
        " 'svm__probability': False,\n",
        " 'svm__random_state': None,\n",
        " 'svm__shrinking': True,\n",
        " 'svm__tol': 0.001,\n",
        " 'svm__verbose': False}"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "penalties = [1, 10, 100, 1000, 10000, 100000]\n",
      "# C_range = 10. ** np.arange(-2, 9)\n",
      "kernels = ['linear', 'poly', 'rbf', 'sigmoid']\n",
      "#Kernel coefficient for \u2018rbf\u2019, \u2018poly\u2019 and \u2018sigmoid\u2019\n",
      "gammas = 10. ** np.arange(-5, 4)\n",
      "\n",
      "param_grid = [\n",
      "  {'svm__C': penalties, 'svm__kernel': ['linear']},\n",
      "  {'svm__C': penalties, 'svm__gamma': gammas, 'svm__kernel': ['rbf', 'poly', 'sigmoid']}\n",
      "]\n",
      "\n",
      "estimators = [('norm', preprocessing.StandardScaler()), ('svm', SVC(C=1.0, kernel='linear', probability=False))]\n",
      "model = Pipeline(estimators)\n",
      "model.fit(X, y)\n",
      "grid = GridSearchCV(model, param_grid=param_grid, cv=KFold(len(X), n_folds=10, shuffle=True, random_state=0) )\n",
      "grid.fit(X, y)\n",
      "print \"Best Classifier:{0}\".format(grid.best_estimator_)\n",
      "print \"Best Score: {0}\".format(grid.best_score_)\n",
      "print \"Best Parameters: {0}\".format(grid.best_params_)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Best Classifier:Pipeline(steps=[('norm', StandardScaler(copy=True, with_mean=True, with_std=True)), ('svm', SVC(C=10, cache_size=200, class_weight=None, coef0=0.0, degree=3, gamma=0.01,\n",
        "  kernel='sigmoid', max_iter=-1, probability=False, random_state=None,\n",
        "  shrinking=True, tol=0.001, verbose=False))])\n",
        "Best Score: 0.988764044944\n",
        "Best Parameters: {'svm__C': 10, 'svm__kernel': 'sigmoid', 'svm__gamma': 0.01}\n"
       ]
      }
     ],
     "prompt_number": 27
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The best parameters are when C=10, kernel is of type 'sigmoid' and gamma=0.01, which yields a score of ~0.989 based on normalizing the data via the pipeline (to avoid extremes), and C is not extremely high so it won't have to work too hard to find the margin.  The best parameters also seemed to be affected by the corresponding CV.  Note that we purposely removed penalties for C <1."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    }
   ],
   "metadata": {}
  }
 ]
}